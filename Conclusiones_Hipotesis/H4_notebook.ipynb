{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hipótesis 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como fue descrito con anterioridad, la tarjeta de datos empleada en la comprobación de la hipótesis contenía los datos referidos al producto interior bruto (PIB) de multitud de países, representando su evolución económica, y datos que reflejan las cantidades de vehículos eléctricos en dichas zonas. El intervalo final de tiempo que se estableció para llevar a cabo los análisis comienza en 2017 y termina en 2022. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McDNykXnR9Y6"
      },
      "source": [
        "## Imports básicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rK_pQ5H42sP"
      },
      "outputs": [],
      "source": [
        "# 1. Imports básicos\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import sklearn\n",
        "import warnings\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import cluster, stats\n",
        "from sklearn import preprocessing\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "scaler = preprocessing.StandardScaler()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0zuqMGWSBYa"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Qc-6tx35Xzn"
      },
      "outputs": [],
      "source": [
        "# 2. Carga de datos\n",
        "\n",
        "ruta_actual = os.getcwd()\n",
        "directorio_superior = os.path.dirname(ruta_actual)\n",
        "sys.path.append(directorio_superior)\n",
        "from Acceso_BBDD.MetodosBBDD import *\n",
        "\n",
        "os.chdir(directorio_superior)\n",
        "dataframe = obtener_dataframe_sql('Hipotesis_4', GOLD)\n",
        "dataframe = dataframe.set_index('Country')\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl9Q0p8tSNbH"
      },
      "source": [
        "## Análisis de Correlación entre variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo primero que se decidió fue revisar la correlación existente entre las variables (o columnas de la tarjeta de datos) con las que se trabajaría. Los resultados obtenidos denotaban una total relación entre las cifras económicas de PIB de los países, pues casi en todas las combinaciones de pares de ellas el coeficiente calculado era 1 o una cantidad muy cercana. Esto tiene sentido al pensar que rara vez el crecimiento o decrecimiento de una zona geográfica en términos monetarios sufre modificaciones aleatorias o repentinas de forma representativa. Por otro lado, en el caso de las variables enfocadas en ventas de vehículos eléctricos ocurría todo lo contrario (valores casi nulos), por lo que queda clara la ausencia de ningún tipo de tendencia social que afecte a este sector del mercado. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZGaZWua7Igf"
      },
      "outputs": [],
      "source": [
        "# 3. Análisis de Correlación entre variables\n",
        "\n",
        "correlation_matrix = dataframe.corr()\n",
        "sns.set(style=\"white\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
        "plt.title(\"Matriz de Correlación entre todas las variables iniciales\")\n",
        "plt.xlabel(\"Variables\")\n",
        "plt.ylabel(\"Variables\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWLqqHMjSzUe"
      },
      "source": [
        "## Análisis de Componentes Principales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Luego, como aproximación a los pasos posteriores y para seguir recolectando información potencialmente útil sobre el conjunto de datos, se extrajo los componentes principales de ellos. Esta técnica busca la proyección según la cual los datos queden mejor representados en términos de mínimos cuadrados, de tal forma que es viable la reducción de dimensionalidad de los dataset mientras se ordenan las restantes según su importancia. En concreto fueron solamente dos el número de características con las que se trabajó, pues ellas podrían después ser graficadas de forma cómoda y sencilla. De esta manera se probó que ciertos países sobresalían del conjunto restante como es el caso de China, EEUU o Luxemburgo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mmfCjYoS1WI"
      },
      "outputs": [],
      "source": [
        "# 4. Análisis de Componentes Principales de todo el Dataframe\n",
        "\n",
        "states = scaler.fit_transform(dataframe)\n",
        "estimator = PCA (n_components = 2)\n",
        "X_pca = estimator.fit_transform(states)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.title(\"Análisis de Componentes Principales de todo el Dataframe\")\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], s=50)\n",
        "for i in range(len(X_pca)):\n",
        "    plt.text(X_pca[i, 0], X_pca[i, 1], dataframe.iloc[i, :].name)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tras estos dos acercamientos, se tomó la decisión de que el uso de técnicas de clustering podría suponer un buen medio para resolver el problema y arrojar pruebas visuales que confirmaran o desmintieran que “Se venden más vehículos en países con más PIB per cápita”: se pensó en representar gráficamente separados por grupos (o clusters) los países estudiados tanto en el ámbito económico como de venta de coches eléctricos. Así resultaría evidente si el grupo de aquellos con mejor estadísticas monetarias contarían a su vez con las más altas tasas de adquisiciones de vehículos. \n",
        "\n",
        "Con ese objetivo en mente, se separó la tarjeta de datos en dos diferentes dataframes nuevamente, uno para el producto interior bruto y el otro para ventas de coches. Además, tuvo que llevar a cabo un proceso de normalización sobre ambos para evitar problemas derivados de órdenes de dimensión diferentes: fue elegido un MinMaxScaler, capaz de transformar a una escala con mínimo = 0 y máximo = 1. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxNo2EEzSWwJ"
      },
      "source": [
        "## Separación y normalización de datos sobre PIB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQpL1VsS5tcZ"
      },
      "outputs": [],
      "source": [
        "# 5. Separación y normalización de datos sobre PIB\n",
        "\n",
        "df_PIB = dataframe[['PIB_2017','PIB_2018','PIB_2019','PIB_2020','PIB_2021','PIB_2022']]\n",
        "datanorm_PIB = min_max_scaler.fit_transform(df_PIB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p005fdsSbRf"
      },
      "source": [
        "## Separación y normalización de datos sobre Vehículos Vendidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DVwvIcb6RsK"
      },
      "outputs": [],
      "source": [
        "# 6. Separación y normalización de datos sobre Vehículos Vendidos\n",
        "\n",
        "df_Coches_Vendidos = dataframe[['CochesVendidos_2017','CochesVendidos_2018','CochesVendidos_2019','CochesVendidos_2020','CochesVendidos_2021','CochesVendidos_2022']]\n",
        "datanorm_Coches_Vendidos = min_max_scaler.fit_transform(df_Coches_Vendidos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfHjfL0wOXFO"
      },
      "source": [
        "## Cálculo de las matrices de similitud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Posteriormente a este procesado y tras la documentación sobre modalidades de clustering vistas durante el curso, se pensó que sería de gran ayuda en el futuro contar tanto con los componentes principales de los dos dataframes generados como con la matriz de similitud. Al igual que antes, en el primer caso se extrajo sólo dos componentes y volvió a emplearse un StandardScaler debido a que al transformar los datos de manera que tengan una media de 0 y una desviación estándar de 1 se evita que ciertas características dominen sobre otras simplemente debido a sus unidades o magnitudes. En el segundo caso la construcción de dichas matrices surgió por la necesidad de comprobar cómo de parecidos eran entre sí los valores de las variables usadas (se utiliza distancia euclídea para ello) para más adelante confeccionar los cluster de manera exacta.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kBykjL1OXze"
      },
      "outputs": [],
      "source": [
        "def calcular_matriz_similitud(datanorm, nombre_grafica):\n",
        "    # 7. Obtención de Componentes Principales sobre el dataframe normalizado introducido y Similarity Matrix\n",
        "\n",
        "    states_datanorm = scaler.fit_transform(datanorm)\n",
        "    estimator = PCA (n_components = 2)\n",
        "    pca_dataframe = estimator.fit_transform(states_datanorm)\n",
        "\n",
        "    dist = sklearn.metrics.DistanceMetric.get_metric('euclidean')\n",
        "    matsim = dist.pairwise(datanorm)\n",
        "    ax = sns.heatmap(matsim,vmin=0, vmax=1)\n",
        "    plt.show()\n",
        "\n",
        "    return matsim, pca_dataframe\n",
        "\n",
        "\n",
        "matsim_pib, PIB_pca = calcular_matriz_similitud(datanorm_PIB, \"matriz_similitud_PIB\")\n",
        "matsim_coches_vendidos, Coches_Vendidos_pca  = calcular_matriz_similitud(datanorm_Coches_Vendidos, \"matriz_similitud_Vehiculos_Vendidos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como información adicional que apoya la teoría propuesta al analizar las correlaciones de las variables, la matriz de similitud de los datos sobre PIB contiene tonos mucho más claros que los oscuros expuestos en la matriz referida a vehículos, lo que denota mayor relación o parecido. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_KbKi1mO78w"
      },
      "source": [
        "## Implementación de una forma de representar los clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con el fin de contrastar diferentes enfoques proporcionados por las técnicas de clustering estudiadas durante el curso decidió implementarse tres de ellas. De esa forma se vio la posibilidad de obtener el mismo resultado (mejor opción posible pues quedaría la hipótesis podría aceptarse o rechazarse directamente) o al menos seleccionar el más lógico y cercano a la realidad. \n",
        "\n",
        " \n",
        "\n",
        "Todos esos enfoques coincidieron en declarar como cantidad óptima de clusters o clases el 2. Esto abrió la puerta a dividir los países estudiados en únicamente dos extremos: aquellos con más o menos poder adquisitivo (para clusters generados a partir del dataset sobre el PIB); y aquellos punteros o no en compras de vehículos eléctricos (para clusters generados a partir del dataset restante). Esto facilitó las futuras interpretaciones, pues desde ese momento bastaba con observar si en las agrupaciones creadas de países los que estaban en un extremo para un dataset lo estaban también en el otro (más ricos junto con más compras de coches y viceversa). \n",
        "\n",
        " \n",
        "\n",
        "Es digno de mención que se empleó como métrica de evaluación de clusters el coeficiente de Silhouette, que posee un rango de -1 a 1, representando valores cercanos a este último que los objetos (países en este caso) están bien ajustados a su propio cluster y, al mismo tiempo, están separados de otros clusters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnk0YMcKPAOO"
      },
      "outputs": [],
      "source": [
        "def representar_clusters(dataframe, labels, num_inicial_labels, pca_dataframe, nombre_grafica):\n",
        "    # 8. Algoritmo para representar los cluster generados\n",
        "\n",
        "    colores = []\n",
        "    for label in labels:\n",
        "        if label == num_inicial_labels:\n",
        "            colores.append(\"blue\")\n",
        "        else:\n",
        "            colores.append(\"green\")\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.title(nombre_grafica)\n",
        "    plt.scatter(pca_dataframe[:,0], pca_dataframe[:,1], c=colores,s=50)\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        if labels[i] != num_inicial_labels:\n",
        "            plt.text(pca_dataframe[i, 0], pca_dataframe[i, 1], dataframe.iloc[i, :].name)\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMqEBKzmPf9D"
      },
      "source": [
        "## Clustering Jerárquico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se comenzó por este tipo de clustering. Para el dataset sobre PIB arrojó dos agrupaciones o clusters con un coeficiente de Silhouette de 0’656, mientras que para el de Vehículos Vendidos un valor de 0’934."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1NdgNEDPhwm"
      },
      "outputs": [],
      "source": [
        "def clustering_jerarquico(dataframe, datanorm, matsim, pca_dataframe, nombre_grafica):\n",
        "    # 9. Clustering Jerárquico sobre el PIB o los Vehiculos Vendidos\n",
        "\n",
        "    best_silhouette = -1\n",
        "    best_cut = -1\n",
        "    best_method = \"\"\n",
        "\n",
        "    # Prueba diferentes valores de cut y métodos de enlace\n",
        "    for cut_value in range(1,  6):  # Prueba diferentes valores de cut\n",
        "        for method in ['single', 'complete', 'average', 'ward']:  # Prueba diferentes métodos de enlace\n",
        "            clusters = cluster.hierarchy.linkage(matsim, method=method)\n",
        "            labels = cluster.hierarchy.fcluster(clusters, cut_value, criterion='distance')\n",
        "            silhouette = metrics.silhouette_score(datanorm, labels)\n",
        "\n",
        "            if silhouette > best_silhouette:\n",
        "                best_silhouette = silhouette\n",
        "                best_cut = cut_value\n",
        "                best_method = method\n",
        "\n",
        "    print('Best cut value: %d' % best_cut)\n",
        "    print('Best linkage method: %s' % best_method)\n",
        "    print('Best Silhouette Coefficient: %0.3f' % best_silhouette)\n",
        "\n",
        "    clusters = cluster.hierarchy.linkage(matsim, method=best_method)\n",
        "    labels = cluster.hierarchy.fcluster(clusters, best_cut , criterion = 'distance')\n",
        "\n",
        "    representar_clusters(dataframe, labels, min(labels), pca_dataframe, nombre_grafica)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtJCluN4VV7t"
      },
      "source": [
        "### Clustering Jerárquico sobre PIB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqJt1CZiRUQh"
      },
      "outputs": [],
      "source": [
        "clustering_jerarquico(df_PIB, datanorm_PIB, matsim_pib, PIB_pca, \"clustering_jerarquico_PIB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYlKmaD3e7CQ"
      },
      "source": [
        "### Clustering Jerárquico sobre Vehículos Vendidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkVwuYPpP0g-"
      },
      "outputs": [],
      "source": [
        "clustering_jerarquico(df_Coches_Vendidos, datanorm_Coches_Vendidos, matsim_coches_vendidos, Coches_Vendidos_pca, \"clustering_jerarquico_Vehiculos_Vendidos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En términos económicos, es curioso cómo se clasifican los países: únicamente se introduce dentro del grupo de los que mejores datos de producto interior bruto a Luxemburgo. Mientras tanto, en términos de adquisiciones de vehículos eléctricos destacan por encima del resto China y EEUU, algo más esperable. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBhWlpAYQR6L"
      },
      "source": [
        "## Clustering K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para el dataset sobre PIB arrojó dos agrupaciones o clusters con un coeficiente de Silhouette de 0’638 (ligeramente menor que antes), mientras que para el de Vehículos Vendidos un valor de 0’934 (exactamente igual que antes). Ambos resultados volvieron a ser bastante prometedores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTyxweD0Qace"
      },
      "outputs": [],
      "source": [
        "def clustering_k_means(dataframe, pca_dataframe, nombre_grafica):\n",
        "    # 10. Clustering K-Means sobre el PIB o los Vehiculos Vendidos\n",
        "\n",
        "    n_clusters_values = range(2, 7)\n",
        "    init_values = ['k-means++', 'random']\n",
        "\n",
        "    best_silhouette = -1\n",
        "    best_params = {}\n",
        "\n",
        "    for n_clusters in n_clusters_values:\n",
        "        for init_method in init_values:\n",
        "            km = KMeans(n_clusters=n_clusters, init=init_method, n_init=10, max_iter=300, tol=0.0001, random_state=42)\n",
        "            labels = km.fit_predict(PIB_pca)\n",
        "            silhouette = metrics.silhouette_score(pca_dataframe, labels)\n",
        "\n",
        "            if silhouette > best_silhouette:\n",
        "                best_silhouette = silhouette\n",
        "                best_params = {'n_clusters': n_clusters, 'init': init_method}\n",
        "\n",
        "    print('Best combination of parameters:')\n",
        "    print(best_params)\n",
        "    print('Best Silhouette Coefficient: %0.3f' % best_silhouette)\n",
        "\n",
        "    km = KMeans(n_clusters=best_params['n_clusters'], init=best_params['init'], n_init=10, max_iter=300, tol=0.0001, random_state=42)\n",
        "    y_km = km.fit_predict(pca_dataframe)\n",
        "\n",
        "    #AD-HOC !!! (para que el cluster de la derecha sea el verde como en el caso del clustering jerárquico)\n",
        "    lista_invertida = []\n",
        "    if nombre_grafica == \"clustering_k_means_PIB\":\n",
        "        lista_invertida = [1 if valor == 0 else 0 for valor in km.labels_]\n",
        "        representar_clusters(dataframe, lista_invertida, min(lista_invertida), pca_dataframe, nombre_grafica)\n",
        "    else:\n",
        "        representar_clusters(dataframe, km.labels_, min(km.labels_), pca_dataframe, nombre_grafica)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Wy3hfc6B7D"
      },
      "source": [
        "### Clustering K-Means sobre PIB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuS6et7eQ1ld"
      },
      "outputs": [],
      "source": [
        "clustering_k_means(df_PIB, PIB_pca, \"clustering_k_means_PIB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-K_XdH-RKcV"
      },
      "source": [
        "### Clustering K-Means sobre Vehículos Vendidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTh-DwADRMpr"
      },
      "outputs": [],
      "source": [
        "clustering_k_means(df_Coches_Vendidos, Coches_Vendidos_pca, \"clustering_k_means_Vehiculos_Vendidos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En términos económicos, esta vez se sitúa a un número más equilibrado de países en cada uno de los dos grandes grupos, algo que es más realista que el caso anterior. Mientras tanto, en términos de adquisiciones de vehículos eléctricos destacan por encima del resto China y EEUU, es decir, lo mismo que ocurría antes y lo que significa que esta clasificación es bastante confiable. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO3bxsfWRbaW"
      },
      "source": [
        "## Clustering Probabilístico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para acabar con el análisis se tomó la decisión de emplear un tercer tipo de clustering, a pesar de ya contar con resultados ciertamente determinantes. A la hora de utilizar esta técnica apareció un problema, y es que al basarse en probabilidad (la aleatoriedad se establece desde el momento mismo de creación de la clase que genera los clusters), los resultados que ofrece pueden variar de una ejecución a otra del código. Esto es lo que ocurre de manera frecuente con las clasificaciones llevadas a cabo sobre el dataset de vehículos vendidos (no en el del PIB, que se mantuvo estable, al menos).  \n",
        "\n",
        " \n",
        "\n",
        "En adición, en este caso no puede presentarse como argumento de fiabilidad el coeficiente de Silhouette debido a que ni es utilizado: simplemente se maneja el “bic” (Bayesian Information Criterion). Este criterio compara el ajuste de diferentes modelos, pero penalizando la complejidad del modelo (número de parámetros), de modo que el modelo con menor valor de BIC debe elegirse. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB8lX3DKR4vs"
      },
      "outputs": [],
      "source": [
        "def clustering_probabilistico(dataframe, pca_dataframe, nombre_grafica):\n",
        "    # 11. Clustering Probabilístico sobre el PIB o los Vehiculos Vendidos\n",
        "\n",
        "    lowest_bic = np.infty\n",
        "    bic = []\n",
        "    best_cv = ''\n",
        "    best_k = -1\n",
        "\n",
        "    cv_types = ['spherical', 'tied', 'diag', 'full']\n",
        "\n",
        "    for cv_type in cv_types:\n",
        "        for k in range(1, 3):\n",
        "            gmm = GaussianMixture(n_components=k, covariance_type=cv_type, init_params='kmeans')\n",
        "            gmm.fit(pca_dataframe)\n",
        "            bic.append(gmm.bic(pca_dataframe))\n",
        "            if bic[-1] < lowest_bic:\n",
        "                lowest_bic = bic[-1]\n",
        "                best_cv = cv_type\n",
        "                best_k = k\n",
        "\n",
        "    print (\"Mejor valor K\", best_k, \"Mejor tipo de Covarianza\", best_cv)\n",
        "\n",
        "    gmm = GaussianMixture(n_components=best_k, covariance_type=best_cv, init_params='random')\n",
        "    gmm.fit(pca_dataframe)\n",
        "    labels =  gmm.predict(pca_dataframe)\n",
        "\n",
        "    #AD-HOC !!! (para que el cluster de la derecha sea el verde como en el caso del clustering jerárquico)\n",
        "    df_aux = dataframe.copy()\n",
        "    df_aux = df_aux.reset_index()\n",
        "    df_aux[\"group\"] = labels\n",
        "    indice = df_aux.index[df_aux['Country'] == 'Argentina']\n",
        "    grupo = labels[indice]\n",
        "\n",
        "    lista_invertida = []\n",
        "    if nombre_grafica == \"clustering_probabilistico_PIB\" and grupo != 0 or nombre_grafica == \"clustering_probabilistico_Vehiculos_Vendidos\" and grupo != 1:\n",
        "        lista_invertida = [1 if valor == 0 else 0 for valor in labels]\n",
        "        representar_clusters(dataframe, lista_invertida, min(lista_invertida), pca_dataframe, nombre_grafica)\n",
        "    else:\n",
        "        representar_clusters(dataframe, labels, min(labels), pca_dataframe, nombre_grafica)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgiqkUVWrkg1"
      },
      "source": [
        "### Clustering Probabilístico sobre PIB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVFtqcnjSIxU"
      },
      "outputs": [],
      "source": [
        "clustering_probabilistico(df_PIB, PIB_pca, \"clustering_probabilistico_PIB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPfqeEA00Sw4"
      },
      "source": [
        "### Clustering Probabilístico sobre Vehículos Vendidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytwD3fliSLpW"
      },
      "outputs": [],
      "source": [
        "clustering_probabilistico(df_Coches_Vendidos, Coches_Vendidos_pca, \"clustering_probabilistico_Vehiculos_Vendidos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusión sobre el clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Posteriormente al estudio de las tres posibilidades presentadas más arriba, se consideró que la más acorde con lo que realmente sucedió es la ofrecida por el Clustering K-Means. No obstante, ni en esa solución ni en las demás hubo una clara relación entre los países clasificados como económicamente sobresalientes y aquellos destacados en ventas de vehículos no contaminantes, por lo que se llegó a la conclusión de que la hipótesis que venía intentándose contrastar debía rechazarse (a pesar de que inicialmente era lógico pensar que sería cierta).  \n",
        "\n",
        " \n",
        "\n",
        "El conocimiento oculto que se desprende de todo lo comentado sobre la hipótesis es que aquellas potencias mundiales con mejores indicadores monetarios son partidarios de realizar inversiones en otros campos diferentes de la automoción eléctrica y sostenible. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrbOI__fStk1"
      },
      "source": [
        "## Contraste de hipótesis adicional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CEmq_yASxDZ"
      },
      "source": [
        "Al poco tiempo de acabar el estudio realizado sobre la tarjeta de datos, se pensó en añadir una última comprobación. Echando un vistazo rápido sobre los datos referidos a la cantidad de coches que no usan la combustión a lo largo de los años manejados, surgieron algunas dudas sobre si realmente con el paso del tiempo había aparecido un incremento en las ventas de ellos o más bien todo lo contrario. \n",
        "\n",
        "En concreto, se lanzó la hipótesis de que las ventas de vehículos eléctricos del año 2019 superaron a las de 2022. A partir de dicho momento, se trabajó para verificar si dicha afirmación era fruto de la mera casualidad (H0) o detrás existían razones de peso que tuvieran como consecuencia ese resultado (H1). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contraste de hipótesis usando muestreo bootstraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def meanBootstrap(dataframe, num_datos):\n",
        "  muestra = [0] * num_datos\n",
        "  for i in range(num_datos):\n",
        "    sample = [dataframe[j] for j in np.random.randint(len(dataframe), size=len(dataframe))]\n",
        "    muestra[i] = np.mean(sample)\n",
        "  return muestra\n",
        "\n",
        "def intervalo_confianza_bootstrap(dataframe, columna_1, columna_2):\n",
        "    # 14. Contraste de hipótesis con muestreo bootstraping\n",
        "\n",
        "    sample_1 = meanBootstrap(dataframe[columna_1], 200)\n",
        "    mean_1 = np.mean(sample_1)\n",
        "    se_1 = np.std(sample_1)\n",
        "\n",
        "    sample_2 = meanBootstrap(dataframe[columna_2], 200)\n",
        "    mean_2 = np.mean(sample_2)\n",
        "    se_2 = np.std(sample_2)\n",
        "\n",
        "    print(\"--- INTERVALOS DE CONFIANZA CON BOOTSTRAP ---\\n\")\n",
        "    print (f\"Media de {columna_1}: {mean_1:6.2f}\")\n",
        "    print(f\"Desviación típica de {columna_1}: {se_1:.2f}\\n\")\n",
        "    print (f\"Media de {columna_2}: {mean_2:6.2f}\")\n",
        "    print(f\"Desviación típica de {columna_2}: {se_2:.2f}\\n\")\n",
        "\n",
        "    print (f\"Intervalo de confianza para {columna_1}: \",[mean_1 - se_1*1.96/ np.sqrt(len(sample_1)), \n",
        "                            mean_1 + se_1*1.96/ np.sqrt(len(sample_1))])\n",
        "    print (f\"Intervalo de confianza para {columna_2}: \",[mean_2 - se_1*1.96/ np.sqrt(len(sample_1)), \n",
        "                            mean_2 + se_2*1.96/ np.sqrt(len(sample_2))],\"\\n\")\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La primera idea que se tuvo fue evitar el uso del muestreo, dado que éste es recomendable para casos en los que se tenga una cantidad tan ingente de datos que suponga un verdadero reto su manejo, algo que para nada ocurría en nuestro dataset (no obstante, se probó de igual forma esa técnica sobre él, confirmando las sospechas de su mejorable funcionamiento).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "intervalo_confianza_bootstrap(dataframe, \"CochesVendidos_2019\", \"CochesVendidos_2022\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contraste de hipótesis sin muestreo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO3RdjBsS3Gq"
      },
      "outputs": [],
      "source": [
        "def intervalos_confianza_columna_completa(dataframe, columna_1, columna_2):\n",
        "    # 13. Contraste de hipótesis sin muestreo\n",
        "\n",
        "    datos_1 = dataframe[columna_1]\n",
        "    datos_2 = dataframe[columna_2]\n",
        "    print(\"\\n--- CONTRASTE DE HIPÓTESIS SIN BOOTSTRAP ---\\n\")\n",
        "    print (f\"Media de {columna_1}: {datos_1.mean():6.2f}\")\n",
        "    print(f\"Desviación típica de {columna_1}: {datos_1.std():.2f}\\n\")\n",
        "    print (f\"Media de {columna_2}: {datos_2.mean():6.2f}\")\n",
        "    print(f\"Desviación típica de {columna_2}: {datos_2.std():.2f}\\n\")\n",
        "\n",
        "    print (f\"Intervalo de confianza para {columna_1}: \",[datos_1.mean() - datos_1.std()*1.96/ np.sqrt(len(datos_1)),\n",
        "                            datos_1.mean() + datos_1.std()*1.96/ np.sqrt(len(datos_1))])\n",
        "    print (f\"Intervalo de confianza para {columna_2}: \",[datos_2.mean() - datos_2.std()*1.96/ np.sqrt(len(datos_2)),\n",
        "                            datos_2.mean() + datos_2.std()*1.96/ np.sqrt(len(datos_2))],\"\\n\")\n",
        "\n",
        "    print(stats.ttest_ind(datos_1, datos_2, equal_var = False), \"\\n\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalmente, y con un grado de garantía del 95%, ciertas conclusiones fueron extraídas. A pesar de que la media de coches vendidos en 2019 sea mayor que la de 2022, la de ambos se sitúa en una zona central del intervalo de confianza del otro. Además, dichos intervalos son bastante similares, de manera que casi podría realizarse una perfecta superposición.  \n",
        "\n",
        " \n",
        "\n",
        "El estudio terminó tras la ejecución de la prueba “T de Student” (manejamos dos años diferentes y sus varianzas son distintas) a través del método ttest_ind() proporcionado por Scipy. El p-value obtenido es muy alto como para considerar lo ocurrido estadísticamente significativo, así que al ser improbable que no haya sucedido por casualidad, se rechaza la hipótesis alternativa formulada inicialmente como H1 y se acepta la nula (H0). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEIfNsIxTPev"
      },
      "outputs": [],
      "source": [
        "intervalos_confianza_columna_completa(dataframe, \"CochesVendidos_2019\", \"CochesVendidos_2022\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QMqEBKzmPf9D",
        "SBhWlpAYQR6L"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
